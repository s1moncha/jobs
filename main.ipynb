{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4163d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "with open('/Users/simoncha/Desktop/projects/jobs/valid_companies.txt', \"r\") as f:\n",
    "    companies = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66e648a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retrieved 52 jobs for Duolingo\n",
      "\n",
      "Total jobs collected: 52\n"
     ]
    }
   ],
   "source": [
    "all_jobs = []\n",
    "\n",
    "for company in companies[2:3]:\n",
    "    company_slug = company.lower().replace(\" \", \"\").replace(\"-\", \"\").replace(\".\", \"\")\n",
    "    url = f\"https://boards-api.greenhouse.io/v1/boards/{company_slug}/jobs\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Failed for: {company} ({response.status_code})\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        for job in data.get(\"jobs\", []):\n",
    "            job_info = {\n",
    "                \"company\": company,\n",
    "                \"title\": job.get(\"title\"),\n",
    "                \"location\": job.get(\"location\", {}).get(\"name\"),\n",
    "                \"url\": job.get(\"absolute_url\"),\n",
    "                \"posted\": job.get(\"first_published\")\n",
    "            }\n",
    "            all_jobs.append(job_info)\n",
    "\n",
    "        print(f\"‚úÖ Retrieved {len(data.get('jobs', []))} jobs for {company}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error for {company}: {e}\")\n",
    "\n",
    "# Optional: Print summary\n",
    "print(f\"\\nTotal jobs collected: {len(all_jobs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0315c8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üá∫üá∏ Found 48 U.S. or U.S.-Remote jobs.\n",
      "üö´ Rejected by lexical filter: 40\n",
      "‚úÖ Passed lexical filter: 8\n",
      "üìâ Auto-NO (embedding < 0.19): 1\n",
      "‚ö° Auto-YES (embedding ‚â• 0.45): 2\n",
      "ü§î Sent to LLM: 5\n",
      "[LLM 1/5] NO ‚Üí Business Development Lead, Japan & Korea\n",
      "[LLM 2/5] NO ‚Üí Influencer Marketing, Intern\n",
      "[LLM 3/5] NO ‚Üí Internal Audit, Intern\n",
      "[LLM 4/5] YES ‚Üí Learning Scientist, Efficacy Research\n",
      "[LLM 5/5] NO ‚Üí Social Content Creator, Intern\n",
      "\n",
      "======================================================================\n",
      "FINAL SUMMARY\n",
      "Total original jobs: 52\n",
      "US-filtered jobs: 48\n",
      "Lexical rejected: 40\n",
      "Embedding rejected: 1\n",
      "LLM YES (apply): 3\n",
      "LLM NO (skip): 4\n",
      "Percent YES overall: 5.77%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import ollama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# =========================================================\n",
    "# 1. LOCATION FILTER (US ONLY)\n",
    "# =========================================================\n",
    "\n",
    "us_states = {\n",
    "    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL',\n",
    "    'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT',\n",
    "    'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI',\n",
    "    'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY'\n",
    "}\n",
    "\n",
    "us_keywords = ['united states', 'us', 'usa']\n",
    "\n",
    "location_exclusions = [\n",
    "    \"australia\", \"emea\", \"united kingdom\", \"india\", \"ind\",\n",
    "    \"deu\", \"dusseldorf\", \"toronto\", \"mexico\"\n",
    "]\n",
    "\n",
    "us_jobs = []\n",
    "\n",
    "for job in all_jobs:\n",
    "    location = job.get(\"location\", \"\")\n",
    "    if not location:\n",
    "        continue\n",
    "\n",
    "    loc_lower = location.lower()\n",
    "\n",
    "    if any(bad in loc_lower for bad in location_exclusions):\n",
    "        continue\n",
    "\n",
    "    if (\n",
    "        any(kw in loc_lower for kw in us_keywords)\n",
    "        or any(state in location for state in us_states)\n",
    "        or \"remote\" in loc_lower\n",
    "    ):\n",
    "        us_jobs.append(job)\n",
    "\n",
    "print(f\"üá∫üá∏ Found {len(us_jobs)} U.S. or U.S.-Remote jobs.\")\n",
    "\n",
    "# =========================================================\n",
    "# 2. LEXICAL SENIORITY FILTER (IMMEDIATELY AFTER LOCATION)\n",
    "# =========================================================\n",
    "\n",
    "EXCLUDE_TERMS = [\n",
    "    \"vp\", \"vice president\", \"svp\", \"evp\",\n",
    "    \"director\", \"sr director\", \"senior director\",\n",
    "    \"head of\", \"principal\", \"staff\",\n",
    "    \"manager\", \"senior manager\", \"area manager\",\n",
    "    \"executive\", \"recruiter\", \"human resources\", \"hr\", \"senior\"\n",
    "]\n",
    "\n",
    "def normalize_title(title):\n",
    "    title = title.lower()\n",
    "    title = re.sub(r\"[^\\w\\s]\", \" \", title)\n",
    "    title = re.sub(r\"\\s+\", \" \", title).strip()\n",
    "    return title\n",
    "\n",
    "def passes_lexical_filter(title):\n",
    "    norm = normalize_title(title)\n",
    "    return not any(term in norm for term in EXCLUDE_TERMS)\n",
    "\n",
    "lexical_pass_jobs = []\n",
    "lexical_rejected_jobs = []\n",
    "\n",
    "for job in us_jobs:\n",
    "    title = job.get(\"title\", \"\")\n",
    "    if not title:\n",
    "        continue\n",
    "\n",
    "    if passes_lexical_filter(title):\n",
    "        lexical_pass_jobs.append(job)\n",
    "    else:\n",
    "        lexical_rejected_jobs.append(job)\n",
    "\n",
    "print(f\"üö´ Rejected by lexical filter: {len(lexical_rejected_jobs)}\")\n",
    "print(f\"‚úÖ Passed lexical filter: {len(lexical_pass_jobs)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 3. EMBEDDING SIMILARITY FILTER\n",
    "# =========================================================\n",
    "\n",
    "TARGET_ROLE_DESCRIPTION = \"\"\"\n",
    "Entry level or new graduate role in data science, analytics,\n",
    "computer science, software engineering, data engineering,\n",
    "applied statistics, or machine learning, based in the United States.\n",
    "\"\"\"\n",
    "\n",
    "# =========================================================\n",
    "# 3. EMBEDDING SIMILARITY + AUTO YES / NO ROUTING\n",
    "# =========================================================\n",
    "\n",
    "EMBEDDING_FLOOR = 0.19     # auto-NO\n",
    "AUTO_YES_SCORE = 0.45     # auto-YES\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "titles = []\n",
    "job_refs = []\n",
    "\n",
    "for job in lexical_pass_jobs:\n",
    "    title = job.get(\"title\", \"\")\n",
    "    if title:\n",
    "        titles.append(title)\n",
    "        job_refs.append(job)\n",
    "\n",
    "# Batch embed titles\n",
    "title_embeddings = embedder.encode(\n",
    "    titles,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "# Embed target role\n",
    "target_embedding = embedder.encode(\n",
    "    [TARGET_ROLE_DESCRIPTION],\n",
    "    normalize_embeddings=True\n",
    ")[0]\n",
    "\n",
    "# Cosine similarity\n",
    "similarities = np.dot(title_embeddings, target_embedding)\n",
    "\n",
    "# Routing buckets\n",
    "auto_yes_jobs = []\n",
    "llm_candidate_jobs = []\n",
    "embedding_rejected_jobs = []\n",
    "\n",
    "for score, job in zip(similarities, job_refs):\n",
    "    job[\"embedding_score\"] = float(score)\n",
    "\n",
    "    if score < EMBEDDING_FLOOR:\n",
    "        embedding_rejected_jobs.append(job)\n",
    "\n",
    "    elif score >= AUTO_YES_SCORE:\n",
    "        auto_yes_jobs.append(job)\n",
    "\n",
    "    else:\n",
    "        llm_candidate_jobs.append(job)\n",
    "\n",
    "print(f\"üìâ Auto-NO (embedding < {EMBEDDING_FLOOR}): {len(embedding_rejected_jobs)}\")\n",
    "print(f\"‚ö° Auto-YES (embedding ‚â• {AUTO_YES_SCORE}): {len(auto_yes_jobs)}\")\n",
    "print(f\"ü§î Sent to LLM: {len(llm_candidate_jobs)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 4. LLM ARBITRATION (ONLY UNCERTAIN BAND)\n",
    "# =========================================================\n",
    "\n",
    "def should_apply(title):\n",
    "    prompt = f\"\"\"\n",
    "You are helping an upcoming graduate in Statistics decide whether to apply for jobs.\n",
    "\n",
    "CRITERIA:\n",
    "- Target roles related to data science, analytics, computer science, software engineering, data engineering, or applied statistics\n",
    "- Entry-level, new grad, or roles that do NOT require extensive experience\n",
    "- Role must plausibly exist in the United States\n",
    "\n",
    "Respond with ONLY one word:\n",
    "YES or NO\n",
    "\n",
    "Role title:\n",
    "{title}\n",
    "\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\"temperature\": 0.0}\n",
    "    )\n",
    "\n",
    "    answer = response[\"message\"][\"content\"].strip().upper()\n",
    "    return \"YES\" if \"YES\" in answer else \"NO\"\n",
    "\n",
    "\n",
    "# Start with auto-YES\n",
    "llm_no_titles = set()\n",
    "yes_jobs = auto_yes_jobs[:]\n",
    "no_jobs = []\n",
    "\n",
    "for i, job in enumerate(llm_candidate_jobs, start=1):\n",
    "    title = job.get(\"title\", \"\")\n",
    "    if not title:\n",
    "        continue\n",
    "\n",
    "    # ---- NEW: skip if already rejected by LLM ----\n",
    "    if title in llm_no_titles:\n",
    "        print(f\"[LLM SKIP] Previously rejected ‚Üí {title}\")\n",
    "        no_jobs.append(job)\n",
    "        continue\n",
    "\n",
    "    decision = should_apply(title)\n",
    "    print(f\"[LLM {i}/{len(llm_candidate_jobs)}] {decision} ‚Üí {title}\")\n",
    "\n",
    "    if decision == \"YES\":\n",
    "        yes_jobs.append(job)\n",
    "    else:\n",
    "        no_jobs.append(job)\n",
    "        llm_no_titles.add(title)   # <-- cache rejection\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. SUMMARY\n",
    "# =========================================================\n",
    "\n",
    "total = len(all_jobs)\n",
    "percent_yes = (len(yes_jobs) / total * 100) if total > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(f\"Total original jobs: {total}\")\n",
    "print(f\"US-filtered jobs: {len(us_jobs)}\")\n",
    "print(f\"Lexical rejected: {len(lexical_rejected_jobs)}\")\n",
    "print(f\"Embedding rejected: {len(embedding_rejected_jobs)}\")\n",
    "print(f\"LLM YES (apply): {len(yes_jobs)}\")\n",
    "print(f\"LLM NO (skip): {len(no_jobs)}\")\n",
    "print(f\"Percent YES overall: {percent_yes:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be806b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 3 jobs to runs/upgrade_20260114_011445.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Columns you want to persist\n",
    "COLUMNS = [\n",
    "    \"company\",\n",
    "    \"title\",\n",
    "    \"location\",\n",
    "    \"url\",\n",
    "    \"posted\",\n",
    "    \"applied\",\n",
    "    \"applied_at\"\n",
    "]\n",
    "\n",
    "# Ensure applied fields exist\n",
    "for job in yes_jobs:\n",
    "    job.setdefault(\"applied\", False)\n",
    "    job.setdefault(\"applied_at\", None)\n",
    "\n",
    "# Filter fields\n",
    "filtered_jobs = [\n",
    "    {k: job.get(k) for k in COLUMNS}\n",
    "    for job in yes_jobs\n",
    "]\n",
    "\n",
    "# Ensure runs/ directory exists\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "\n",
    "# Timestamped filename\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"runs/upgrade_{timestamp}.json\"\n",
    "\n",
    "# Write JSON\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(filtered_jobs, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(filtered_jobs)} jobs to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27959234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Run data committed and pushed to GitHub\n"
     ]
    }
   ],
   "source": [
    "from git import Repo\n",
    "from datetime import datetime\n",
    "\n",
    "repo = Repo(\".\")\n",
    "\n",
    "# Add ALL changes, including untracked files\n",
    "repo.git.add(all=True)\n",
    "\n",
    "repo.index.commit(f\"Add job run ({timestamp})\")\n",
    "\n",
    "repo.remote(name=\"origin\").push()\n",
    "\n",
    "print(\"üöÄ Run data committed and pushed to GitHub\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de92af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
